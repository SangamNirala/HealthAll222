#!/usr/bin/env python3
"""
üöÄ INTELLIGENCE AMPLIFICATION PHASE B WEEKS 4 & 5 COMPREHENSIVE TESTING

Testing the most advanced medical conversation intelligence system ever conceived.
This test suite validates Week 4 Predictive Modeling & Subspecialty Clinical Reasoning
and Week 5 Integration Testing & Clinical Validation implementations.

TESTING SCOPE:
- Week 4: Predictive Intent Modeling, Subspecialty Clinical Reasoning
- Week 5: Integration Testing Framework, Clinical Validation Scenarios
- Performance benchmarking and production readiness assessment
"""

import asyncio
import aiohttp
import json
import time
import sys
from typing import Dict, List, Any, Optional
from datetime import datetime
import traceback

# Test configuration
BACKEND_URL = "https://clinical-ai-4.preview.emergentagent.com/api"
TEST_TIMEOUT = 30  # seconds

class WeekFourFiveTestSuite:
    """Comprehensive test suite for Week 4 & 5 implementations"""
    
    def __init__(self):
        self.session = None
        self.test_results = {
            "week4_predictive_modeling": {},
            "week4_subspecialty_reasoning": {},
            "week5_integration_testing": {},
            "week5_clinical_validation": {},
            "performance_metrics": {},
            "overall_status": "pending"
        }
        self.start_time = time.time()
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=TEST_TIMEOUT))
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def run_comprehensive_tests(self):
        """Run all Week 4 & 5 tests"""
        print("üöÄ STARTING INTELLIGENCE AMPLIFICATION PHASE B WEEKS 4 & 5 COMPREHENSIVE TESTING")
        print("=" * 80)
        
        try:
            # Week 4 Testing
            print("\nüìä WEEK 4 TESTING: PREDICTIVE MODELING & SUBSPECIALTY CLINICAL REASONING")
            await self.test_week4_predictive_intent_modeling()
            await self.test_week4_subspecialty_clinical_reasoning()
            
            # Week 5 Testing  
            print("\nüî¨ WEEK 5 TESTING: INTEGRATION TESTING & CLINICAL VALIDATION")
            await self.test_week5_integration_testing()
            await self.test_week5_clinical_validation()
            
            # Performance Testing
            print("\n‚ö° PERFORMANCE BENCHMARKING")
            await self.test_performance_benchmarking()
            
            # Generate final report
            await self.generate_final_report()
            
        except Exception as e:
            print(f"‚ùå CRITICAL ERROR in comprehensive testing: {str(e)}")
            traceback.print_exc()
            self.test_results["overall_status"] = "failed"
    
    async def test_week4_predictive_intent_modeling(self):
        """Test Week 4 Predictive Intent Modeling capabilities"""
        print("\nüîÆ Testing Week 4 Predictive Intent Modeling...")
        
        test_scenarios = [
            {
                "name": "Complex Multi-Intent Prediction",
                "conversation_history": [
                    {"message": "I have severe chest pain", "intent": "symptom_reporting", "timestamp": "2024-01-15T10:00:00Z"},
                    {"message": "It started about an hour ago", "intent": "duration_inquiry", "timestamp": "2024-01-15T10:01:00Z"},
                    {"message": "The pain is crushing and radiating to my arm", "intent": "severity_assessment", "timestamp": "2024-01-15T10:02:00Z"}
                ],
                "current_context": {
                    "patient_data": {"age": 55, "gender": "male"},
                    "conversation_length": 3,
                    "urgency_level": "high"
                }
            },
            {
                "name": "Neurological Symptom Progression",
                "conversation_history": [
                    {"message": "I have the worst headache of my life", "intent": "symptom_reporting", "timestamp": "2024-01-15T10:00:00Z"},
                    {"message": "It came on suddenly", "intent": "onset_description", "timestamp": "2024-01-15T10:01:00Z"},
                    {"message": "I also have neck stiffness", "intent": "associated_symptoms", "timestamp": "2024-01-15T10:02:00Z"}
                ],
                "current_context": {
                    "patient_data": {"age": 42, "gender": "female"},
                    "conversation_length": 3,
                    "urgency_level": "critical"
                }
            }
        ]
        
        # Test 1: Predictive Intent Modeling Endpoint
        print("  üéØ Testing POST /api/medical-ai/predictive-intent-modeling...")
        
        for scenario in test_scenarios:
            try:
                start_time = time.time()
                
                payload = {
                    "conversation_history": scenario["conversation_history"],
                    "current_context": scenario["current_context"],
                    "prediction_horizon": "next_3_intents",
                    "include_progression_analysis": True
                }
                
                async with self.session.post(
                    f"{BACKEND_URL}/medical-ai/predictive-intent-modeling",
                    json=payload
                ) as response:
                    processing_time = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # Validate response structure
                        required_fields = [
                            "predicted_intents", "progression_analysis", "proactive_responses",
                            "processing_time_ms", "algorithm_version", "prediction_confidence"
                        ]
                        
                        missing_fields = [field for field in required_fields if field not in data]
                        if missing_fields:
                            print(f"    ‚ùå {scenario['name']}: Missing fields: {missing_fields}")
                            continue
                        
                        # Validate prediction accuracy requirements
                        predicted_intents = data.get("predicted_intents", [])
                        if len(predicted_intents) < 3:
                            print(f"    ‚ùå {scenario['name']}: Expected at least 3 predicted intents, got {len(predicted_intents)}")
                            continue
                        
                        # Check processing time target (<25ms)
                        if processing_time > 25:
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Processing time {processing_time:.1f}ms exceeds 25ms target")
                        
                        # Validate algorithm version
                        if data.get("algorithm_version") != "3.1_intelligence_amplification_week4":
                            print(f"    ‚ùå {scenario['name']}: Wrong algorithm version: {data.get('algorithm_version')}")
                            continue
                        
                        # Check prediction confidence (target >90%)
                        avg_confidence = sum(intent.get("confidence_score", 0) for intent in predicted_intents) / len(predicted_intents)
                        if avg_confidence < 0.9:
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Average confidence {avg_confidence:.2f} below 90% target")
                        
                        print(f"    ‚úÖ {scenario['name']}: SUCCESS - {len(predicted_intents)} intents predicted in {processing_time:.1f}ms")
                        
                        # Store detailed results
                        self.test_results["week4_predictive_modeling"][scenario['name']] = {
                            "status": "success",
                            "processing_time_ms": processing_time,
                            "predicted_intents_count": len(predicted_intents),
                            "average_confidence": avg_confidence,
                            "algorithm_version": data.get("algorithm_version")
                        }
                        
                    else:
                        error_text = await response.text()
                        print(f"    ‚ùå {scenario['name']}: HTTP {response.status} - {error_text}")
                        self.test_results["week4_predictive_modeling"][scenario['name']] = {
                            "status": "failed",
                            "error": f"HTTP {response.status}",
                            "details": error_text
                        }
                        
            except Exception as e:
                print(f"    ‚ùå {scenario['name']}: Exception - {str(e)}")
                self.test_results["week4_predictive_modeling"][scenario['name']] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # Test 2: Conversation Intelligence Endpoint
        print("  üß† Testing POST /api/medical-ai/conversation-intelligence...")
        
        try:
            payload = {
                "conversation_history": test_scenarios[0]["conversation_history"],
                "patient_data": test_scenarios[0]["current_context"]["patient_data"],
                "current_context": test_scenarios[0]["current_context"],
                "analysis_depth": "comprehensive"
            }
            
            start_time = time.time()
            async with self.session.post(
                f"{BACKEND_URL}/medical-ai/conversation-intelligence",
                json=payload
            ) as response:
                processing_time = (time.time() - start_time) * 1000
                
                if response.status == 200:
                    data = await response.json()
                    
                    # Validate comprehensive intelligence analysis
                    required_fields = [
                        "predicted_intents", "progression_analysis", "proactive_responses",
                        "conversation_risk_assessment", "engagement_optimization", 
                        "clinical_decision_support", "processing_time_ms"
                    ]
                    
                    missing_fields = [field for field in required_fields if field not in data]
                    if missing_fields:
                        print(f"    ‚ùå Conversation Intelligence: Missing fields: {missing_fields}")
                    else:
                        print(f"    ‚úÖ Conversation Intelligence: SUCCESS - Comprehensive analysis in {processing_time:.1f}ms")
                        self.test_results["week4_predictive_modeling"]["conversation_intelligence"] = {
                            "status": "success",
                            "processing_time_ms": processing_time,
                            "comprehensive_analysis": True
                        }
                else:
                    error_text = await response.text()
                    print(f"    ‚ùå Conversation Intelligence: HTTP {response.status} - {error_text}")
                    
        except Exception as e:
            print(f"    ‚ùå Conversation Intelligence: Exception - {str(e)}")
        
        # Test 3: Performance Metrics Endpoint
        print("  üìà Testing GET /api/medical-ai/predictive-modeling-performance...")
        
        try:
            async with self.session.get(f"{BACKEND_URL}/medical-ai/predictive-modeling-performance") as response:
                if response.status == 200:
                    data = await response.json()
                    
                    required_metrics = [
                        "total_predictions", "average_processing_time_ms", "model_accuracy",
                        "algorithm_version", "system_status"
                    ]
                    
                    missing_metrics = [metric for metric in required_metrics if metric not in data]
                    if missing_metrics:
                        print(f"    ‚ùå Performance Metrics: Missing metrics: {missing_metrics}")
                    else:
                        accuracy = data.get("model_accuracy", 0)
                        avg_time = data.get("average_processing_time_ms", 0)
                        
                        print(f"    ‚úÖ Performance Metrics: Accuracy {accuracy:.1%}, Avg Time {avg_time:.1f}ms")
                        self.test_results["week4_predictive_modeling"]["performance_metrics"] = {
                            "status": "success",
                            "model_accuracy": accuracy,
                            "average_processing_time_ms": avg_time
                        }
                else:
                    error_text = await response.text()
                    print(f"    ‚ùå Performance Metrics: HTTP {response.status} - {error_text}")
                    
        except Exception as e:
            print(f"    ‚ùå Performance Metrics: Exception - {str(e)}")
    
    async def test_week4_subspecialty_clinical_reasoning(self):
        """Test Week 4 Subspecialty Clinical Reasoning Engine"""
        print("\nüè• Testing Week 4 Subspecialty Clinical Reasoning...")
        
        subspecialty_scenarios = [
            {
                "subspecialty": "cardiology",
                "intents": [
                    {"intent_name": "cardiac_chest_pain_assessment", "confidence": 0.95},
                    {"intent_name": "cardiac_symptom_evaluation", "confidence": 0.88}
                ],
                "context": {
                    "patient_data": {"age": 58, "gender": "male", "risk_factors": ["hypertension", "diabetes"]},
                    "message": "I have crushing chest pain radiating to my left arm",
                    "urgency_level": "critical"
                },
                "expected_features": ["cardiac_risk_stratification", "ecg_indications", "biomarker_recommendations"]
            },
            {
                "subspecialty": "neurology", 
                "intents": [
                    {"intent_name": "neurological_symptom_assessment", "confidence": 0.92},
                    {"intent_name": "headache_migraine_evaluation", "confidence": 0.85}
                ],
                "context": {
                    "patient_data": {"age": 45, "gender": "female"},
                    "message": "Sudden severe headache with neck stiffness",
                    "urgency_level": "critical"
                },
                "expected_features": ["stroke_assessment", "neuroimaging_indications", "red_flag_symptoms"]
            },
            {
                "subspecialty": "emergency_medicine",
                "intents": [
                    {"intent_name": "emergency_concern", "confidence": 0.96},
                    {"intent_name": "urgent_scheduling", "confidence": 0.89}
                ],
                "context": {
                    "patient_data": {"age": 35, "gender": "male"},
                    "message": "Severe shortness of breath and chest pain",
                    "urgency_level": "critical"
                },
                "expected_features": ["triage_category", "immediate_interventions", "time_sensitive_protocols"]
            }
        ]
        
        print("  üî¨ Testing POST /api/medical-ai/subspecialty-reasoning...")
        
        for scenario in subspecialty_scenarios:
            try:
                start_time = time.time()
                
                payload = {
                    "subspecialty": scenario["subspecialty"],
                    "intents": scenario["intents"],
                    "context": scenario["context"],
                    "reasoning_depth": "expert_level"
                }
                
                async with self.session.post(
                    f"{BACKEND_URL}/medical-ai/subspecialty-reasoning",
                    json=payload
                ) as response:
                    processing_time = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # Validate subspecialty-specific reasoning
                        subspecialty_key = f"{scenario['subspecialty']}_reasoning"
                        if subspecialty_key not in data:
                            print(f"    ‚ùå {scenario['subspecialty'].title()}: Missing subspecialty reasoning")
                            continue
                        
                        reasoning_data = data[subspecialty_key]
                        
                        # Check for expected subspecialty features
                        missing_features = [
                            feature for feature in scenario["expected_features"] 
                            if feature not in reasoning_data
                        ]
                        
                        if missing_features:
                            print(f"    ‚ùå {scenario['subspecialty'].title()}: Missing features: {missing_features}")
                            continue
                        
                        # Check processing time target (<25ms)
                        if processing_time > 25:
                            print(f"    ‚ö†Ô∏è {scenario['subspecialty'].title()}: Processing time {processing_time:.1f}ms exceeds 25ms target")
                        
                        # Validate subspecialty confidence
                        confidence = reasoning_data.get("subspecialty_confidence", "")
                        if confidence not in ["expert_level", "specialist", "experienced"]:
                            print(f"    ‚ö†Ô∏è {scenario['subspecialty'].title()}: Low subspecialty confidence: {confidence}")
                        
                        print(f"    ‚úÖ {scenario['subspecialty'].title()}: SUCCESS - Expert reasoning in {processing_time:.1f}ms")
                        
                        self.test_results["week4_subspecialty_reasoning"][scenario['subspecialty']] = {
                            "status": "success",
                            "processing_time_ms": processing_time,
                            "subspecialty_confidence": confidence,
                            "features_validated": len(scenario["expected_features"]) - len(missing_features)
                        }
                        
                    else:
                        error_text = await response.text()
                        print(f"    ‚ùå {scenario['subspecialty'].title()}: HTTP {response.status} - {error_text}")
                        self.test_results["week4_subspecialty_reasoning"][scenario['subspecialty']] = {
                            "status": "failed",
                            "error": f"HTTP {response.status}",
                            "details": error_text
                        }
                        
            except Exception as e:
                print(f"    ‚ùå {scenario['subspecialty'].title()}: Exception - {str(e)}")
                self.test_results["week4_subspecialty_reasoning"][scenario['subspecialty']] = {
                    "status": "error",
                    "error": str(e)
                }
    
    async def test_week5_integration_testing(self):
        """Test Week 5 Integration Testing Framework"""
        print("\nüîó Testing Week 5 Integration Testing Framework...")
        
        integration_scenarios = [
            {
                "name": "Complete Pipeline Integration",
                "test_type": "end_to_end_pipeline",
                "medical_scenario": "acute_chest_pain",
                "components_to_test": ["week1", "week2", "week3", "week4"],
                "performance_targets": {
                    "total_processing_time_ms": 30,
                    "accuracy_threshold": 0.95
                }
            },
            {
                "name": "Neurological Emergency Integration",
                "test_type": "subspecialty_integration", 
                "medical_scenario": "acute_stroke_symptoms",
                "components_to_test": ["intent_classification", "multi_intent_orchestration", "subspecialty_reasoning"],
                "performance_targets": {
                    "total_processing_time_ms": 25,
                    "clinical_accuracy": 0.90
                }
            }
        ]
        
        print("  üß™ Testing POST /api/medical-ai/integration-testing...")
        
        for scenario in integration_scenarios:
            try:
                start_time = time.time()
                
                payload = {
                    "test_scenario": scenario["medical_scenario"],
                    "integration_type": scenario["test_type"],
                    "components_to_test": scenario["components_to_test"],
                    "performance_targets": scenario["performance_targets"],
                    "validation_level": "comprehensive"
                }
                
                async with self.session.post(
                    f"{BACKEND_URL}/medical-ai/integration-testing",
                    json=payload
                ) as response:
                    processing_time = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # Validate integration test results
                        required_fields = [
                            "integration_results", "component_performance", "pipeline_metrics",
                            "clinical_accuracy_validation", "performance_benchmarking"
                        ]
                        
                        missing_fields = [field for field in required_fields if field not in data]
                        if missing_fields:
                            print(f"    ‚ùå {scenario['name']}: Missing fields: {missing_fields}")
                            continue
                        
                        # Check pipeline performance
                        pipeline_metrics = data.get("pipeline_metrics", {})
                        total_time = pipeline_metrics.get("total_processing_time_ms", 0)
                        target_time = scenario["performance_targets"]["total_processing_time_ms"]
                        
                        if total_time > target_time:
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Pipeline time {total_time:.1f}ms exceeds {target_time}ms target")
                        
                        # Check clinical accuracy
                        accuracy_validation = data.get("clinical_accuracy_validation", {})
                        clinical_accuracy = accuracy_validation.get("overall_accuracy", 0)
                        target_accuracy = scenario["performance_targets"].get("accuracy_threshold", 0.95)
                        
                        if clinical_accuracy < target_accuracy:
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Clinical accuracy {clinical_accuracy:.2%} below {target_accuracy:.2%} target")
                        
                        print(f"    ‚úÖ {scenario['name']}: SUCCESS - Pipeline integration validated in {processing_time:.1f}ms")
                        
                        self.test_results["week5_integration_testing"][scenario['name']] = {
                            "status": "success",
                            "processing_time_ms": processing_time,
                            "pipeline_time_ms": total_time,
                            "clinical_accuracy": clinical_accuracy,
                            "components_tested": len(scenario["components_to_test"])
                        }
                        
                    else:
                        error_text = await response.text()
                        print(f"    ‚ùå {scenario['name']}: HTTP {response.status} - {error_text}")
                        self.test_results["week5_integration_testing"][scenario['name']] = {
                            "status": "failed",
                            "error": f"HTTP {response.status}",
                            "details": error_text
                        }
                        
            except Exception as e:
                print(f"    ‚ùå {scenario['name']}: Exception - {str(e)}")
                self.test_results["week5_integration_testing"][scenario['name']] = {
                    "status": "error",
                    "error": str(e)
                }
    
    async def test_week5_clinical_validation(self):
        """Test Week 5 Clinical Validation Scenarios"""
        print("\nüè• Testing Week 5 Clinical Validation Scenarios...")
        
        clinical_scenarios = [
            {
                "name": "Emergency Medicine - STEMI",
                "specialty": "emergency_medicine",
                "scenario_type": "acute_myocardial_infarction",
                "patient_presentation": {
                    "chief_complaint": "Severe crushing chest pain for 2 hours",
                    "vital_signs": {"bp": "160/90", "hr": 110, "rr": 22},
                    "symptoms": ["chest_pain", "diaphoresis", "nausea"],
                    "risk_factors": ["smoking", "hypertension", "family_history"]
                },
                "expected_outcomes": {
                    "urgency_classification": "critical",
                    "clinical_accuracy_score": 0.95,
                    "safety_score": 0.98
                }
            },
            {
                "name": "Neurology - Acute Stroke",
                "specialty": "neurology",
                "scenario_type": "acute_ischemic_stroke",
                "patient_presentation": {
                    "chief_complaint": "Sudden weakness and speech difficulty",
                    "vital_signs": {"bp": "180/100", "hr": 88, "rr": 18},
                    "symptoms": ["facial_drooping", "arm_weakness", "speech_difficulty"],
                    "onset_time": "45_minutes_ago"
                },
                "expected_outcomes": {
                    "urgency_classification": "critical",
                    "clinical_accuracy_score": 0.92,
                    "safety_score": 0.96
                }
            },
            {
                "name": "Cardiology - Heart Failure",
                "specialty": "cardiology",
                "scenario_type": "acute_decompensated_heart_failure",
                "patient_presentation": {
                    "chief_complaint": "Shortness of breath and leg swelling",
                    "vital_signs": {"bp": "140/85", "hr": 95, "rr": 24},
                    "symptoms": ["dyspnea", "orthopnea", "peripheral_edema"],
                    "medical_history": ["previous_heart_failure", "diabetes"]
                },
                "expected_outcomes": {
                    "urgency_classification": "high",
                    "clinical_accuracy_score": 0.88,
                    "safety_score": 0.94
                }
            }
        ]
        
        print("  üî¨ Testing POST /api/medical-ai/clinical-validation...")
        
        for scenario in clinical_scenarios:
            try:
                start_time = time.time()
                
                payload = {
                    "clinical_scenario": scenario["scenario_type"],
                    "specialty": scenario["specialty"],
                    "patient_presentation": scenario["patient_presentation"],
                    "validation_criteria": {
                        "clinical_accuracy_weight": 0.4,
                        "urgency_assessment_weight": 0.35,
                        "clinical_context_weight": 0.25
                    },
                    "safety_assessment": True
                }
                
                async with self.session.post(
                    f"{BACKEND_URL}/medical-ai/clinical-validation",
                    json=payload
                ) as response:
                    processing_time = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # Validate clinical validation results
                        required_fields = [
                            "clinical_accuracy_score", "urgency_assessment", "safety_assessment",
                            "subspecialty_validation", "clinical_reasoning_quality"
                        ]
                        
                        missing_fields = [field for field in required_fields if field not in data]
                        if missing_fields:
                            print(f"    ‚ùå {scenario['name']}: Missing fields: {missing_fields}")
                            continue
                        
                        # Check clinical accuracy
                        clinical_accuracy = data.get("clinical_accuracy_score", 0)
                        expected_accuracy = scenario["expected_outcomes"]["clinical_accuracy_score"]
                        
                        if clinical_accuracy < expected_accuracy:
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Clinical accuracy {clinical_accuracy:.2%} below expected {expected_accuracy:.2%}")
                        
                        # Check safety score
                        safety_assessment = data.get("safety_assessment", {})
                        safety_score = safety_assessment.get("overall_safety_score", 0)
                        expected_safety = scenario["expected_outcomes"]["safety_score"]
                        
                        if safety_score < expected_safety:
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Safety score {safety_score:.2%} below expected {expected_safety:.2%}")
                        
                        # Check urgency classification
                        urgency_assessment = data.get("urgency_assessment", {})
                        urgency_classification = urgency_assessment.get("classification", "")
                        expected_urgency = scenario["expected_outcomes"]["urgency_classification"]
                        
                        if urgency_classification.lower() != expected_urgency.lower():
                            print(f"    ‚ö†Ô∏è {scenario['name']}: Urgency '{urgency_classification}' != expected '{expected_urgency}'")
                        
                        print(f"    ‚úÖ {scenario['name']}: SUCCESS - Clinical validation completed in {processing_time:.1f}ms")
                        print(f"        Accuracy: {clinical_accuracy:.1%}, Safety: {safety_score:.1%}, Urgency: {urgency_classification}")
                        
                        self.test_results["week5_clinical_validation"][scenario['name']] = {
                            "status": "success",
                            "processing_time_ms": processing_time,
                            "clinical_accuracy_score": clinical_accuracy,
                            "safety_score": safety_score,
                            "urgency_classification": urgency_classification
                        }
                        
                    else:
                        error_text = await response.text()
                        print(f"    ‚ùå {scenario['name']}: HTTP {response.status} - {error_text}")
                        self.test_results["week5_clinical_validation"][scenario['name']] = {
                            "status": "failed",
                            "error": f"HTTP {response.status}",
                            "details": error_text
                        }
                        
            except Exception as e:
                print(f"    ‚ùå {scenario['name']}: Exception - {str(e)}")
                self.test_results["week5_clinical_validation"][scenario['name']] = {
                    "status": "error",
                    "error": str(e)
                }
    
    async def test_performance_benchmarking(self):
        """Test performance benchmarking endpoints"""
        print("\n‚ö° Testing Performance Benchmarking...")
        
        # Test performance benchmarking endpoint
        print("  üìä Testing POST /api/medical-ai/performance-benchmarking...")
        
        try:
            payload = {
                "benchmark_type": "comprehensive",
                "test_scenarios": ["emergency", "routine", "subspecialty"],
                "performance_targets": {
                    "processing_time_ms": 30,
                    "accuracy_threshold": 0.95,
                    "safety_score_threshold": 0.90
                },
                "concurrent_requests": 10
            }
            
            start_time = time.time()
            async with self.session.post(
                f"{BACKEND_URL}/medical-ai/performance-benchmarking",
                json=payload
            ) as response:
                processing_time = (time.time() - start_time) * 1000
                
                if response.status == 200:
                    data = await response.json()
                    
                    # Validate benchmarking results
                    required_fields = [
                        "benchmark_results", "performance_metrics", "stress_test_results",
                        "system_health_assessment", "production_readiness_score"
                    ]
                    
                    missing_fields = [field for field in required_fields if field not in data]
                    if missing_fields:
                        print(f"    ‚ùå Performance Benchmarking: Missing fields: {missing_fields}")
                    else:
                        performance_metrics = data.get("performance_metrics", {})
                        avg_processing_time = performance_metrics.get("average_processing_time_ms", 0)
                        production_readiness = data.get("production_readiness_score", 0)
                        
                        print(f"    ‚úÖ Performance Benchmarking: SUCCESS")
                        print(f"        Avg Processing Time: {avg_processing_time:.1f}ms")
                        print(f"        Production Readiness: {production_readiness:.1%}")
                        
                        self.test_results["performance_metrics"]["benchmarking"] = {
                            "status": "success",
                            "average_processing_time_ms": avg_processing_time,
                            "production_readiness_score": production_readiness
                        }
                else:
                    error_text = await response.text()
                    print(f"    ‚ùå Performance Benchmarking: HTTP {response.status} - {error_text}")
                    
        except Exception as e:
            print(f"    ‚ùå Performance Benchmarking: Exception - {str(e)}")
        
        # Test Week 5 integration performance endpoint
        print("  üìà Testing GET /api/medical-ai/week5-integration-performance...")
        
        try:
            async with self.session.get(f"{BACKEND_URL}/medical-ai/week5-integration-performance") as response:
                if response.status == 200:
                    data = await response.json()
                    
                    required_metrics = [
                        "integration_performance", "pipeline_metrics", "clinical_validation_stats",
                        "system_health", "week5_capabilities"
                    ]
                    
                    missing_metrics = [metric for metric in required_metrics if metric not in data]
                    if missing_metrics:
                        print(f"    ‚ùå Week 5 Performance: Missing metrics: {missing_metrics}")
                    else:
                        pipeline_metrics = data.get("pipeline_metrics", {})
                        total_pipeline_time = pipeline_metrics.get("total_processing_time_ms", 0)
                        
                        print(f"    ‚úÖ Week 5 Integration Performance: SUCCESS")
                        print(f"        Total Pipeline Time: {total_pipeline_time:.1f}ms")
                        
                        self.test_results["performance_metrics"]["week5_integration"] = {
                            "status": "success",
                            "total_pipeline_time_ms": total_pipeline_time
                        }
                else:
                    error_text = await response.text()
                    print(f"    ‚ùå Week 5 Performance: HTTP {response.status} - {error_text}")
                    
        except Exception as e:
            print(f"    ‚ùå Week 5 Performance: Exception - {str(e)}")
    
    async def generate_final_report(self):
        """Generate comprehensive final test report"""
        print("\n" + "=" * 80)
        print("üìã FINAL TEST REPORT - INTELLIGENCE AMPLIFICATION PHASE B WEEKS 4 & 5")
        print("=" * 80)
        
        total_time = time.time() - self.start_time
        
        # Calculate success rates
        week4_predictive_success = sum(1 for result in self.test_results["week4_predictive_modeling"].values() 
                                     if isinstance(result, dict) and result.get("status") == "success")
        week4_predictive_total = len(self.test_results["week4_predictive_modeling"])
        
        week4_subspecialty_success = sum(1 for result in self.test_results["week4_subspecialty_reasoning"].values() 
                                       if isinstance(result, dict) and result.get("status") == "success")
        week4_subspecialty_total = len(self.test_results["week4_subspecialty_reasoning"])
        
        week5_integration_success = sum(1 for result in self.test_results["week5_integration_testing"].values() 
                                      if isinstance(result, dict) and result.get("status") == "success")
        week5_integration_total = len(self.test_results["week5_integration_testing"])
        
        week5_clinical_success = sum(1 for result in self.test_results["week5_clinical_validation"].values() 
                                   if isinstance(result, dict) and result.get("status") == "success")
        week5_clinical_total = len(self.test_results["week5_clinical_validation"])
        
        # Calculate overall success rate
        total_success = week4_predictive_success + week4_subspecialty_success + week5_integration_success + week5_clinical_success
        total_tests = week4_predictive_total + week4_subspecialty_total + week5_integration_total + week5_clinical_total
        overall_success_rate = (total_success / total_tests * 100) if total_tests > 0 else 0
        
        print(f"\nüéØ OVERALL RESULTS:")
        print(f"   Total Tests: {total_tests}")
        print(f"   Successful: {total_success}")
        print(f"   Success Rate: {overall_success_rate:.1f}%")
        print(f"   Total Testing Time: {total_time:.1f}s")
        
        print(f"\nüìä WEEK 4 PREDICTIVE MODELING & SUBSPECIALTY REASONING:")
        if week4_predictive_total > 0:
            print(f"   Predictive Modeling: {week4_predictive_success}/{week4_predictive_total} ({week4_predictive_success/week4_predictive_total*100:.1f}%)")
        if week4_subspecialty_total > 0:
            print(f"   Subspecialty Reasoning: {week4_subspecialty_success}/{week4_subspecialty_total} ({week4_subspecialty_success/week4_subspecialty_total*100:.1f}%)")
        
        print(f"\nüî¨ WEEK 5 INTEGRATION TESTING & CLINICAL VALIDATION:")
        if week5_integration_total > 0:
            print(f"   Integration Testing: {week5_integration_success}/{week5_integration_total} ({week5_integration_success/week5_integration_total*100:.1f}%)")
        if week5_clinical_total > 0:
            print(f"   Clinical Validation: {week5_clinical_success}/{week5_clinical_total} ({week5_clinical_success/week5_clinical_total*100:.1f}%)")
        
        # Performance summary
        print(f"\n‚ö° PERFORMANCE SUMMARY:")
        performance_data = []
        
        for category, results in self.test_results.items():
            if isinstance(results, dict):
                for test_name, result in results.items():
                    if isinstance(result, dict) and "processing_time_ms" in result:
                        performance_data.append(result["processing_time_ms"])
        
        if performance_data:
            avg_performance = sum(performance_data) / len(performance_data)
            max_performance = max(performance_data)
            print(f"   Average Processing Time: {avg_performance:.1f}ms")
            print(f"   Maximum Processing Time: {max_performance:.1f}ms")
            print(f"   Target Processing Time: <30ms (Pipeline), <25ms (Individual)")
        
        # Production readiness assessment
        print(f"\nüöÄ PRODUCTION READINESS ASSESSMENT:")
        
        if overall_success_rate >= 90:
            readiness_status = "READY FOR PRODUCTION"
            status_emoji = "‚úÖ"
        elif overall_success_rate >= 75:
            readiness_status = "READY WITH MINOR ISSUES"
            status_emoji = "‚ö†Ô∏è"
        else:
            readiness_status = "NOT READY - REQUIRES FIXES"
            status_emoji = "‚ùå"
        
        print(f"   {status_emoji} Status: {readiness_status}")
        print(f"   Overall Success Rate: {overall_success_rate:.1f}%")
        
        # Set overall status
        if overall_success_rate >= 75:
            self.test_results["overall_status"] = "success"
        else:
            self.test_results["overall_status"] = "failed"
        
        print(f"\nüìù DETAILED RESULTS:")
        print(json.dumps(self.test_results, indent=2, default=str))
        
        print("\n" + "=" * 80)
        print("üèÅ INTELLIGENCE AMPLIFICATION PHASE B WEEKS 4 & 5 TESTING COMPLETE")
        print("=" * 80)

async def main():
    """Main test execution function"""
    async with WeekFourFiveTestSuite() as test_suite:
        await test_suite.run_comprehensive_tests()

if __name__ == "__main__":
    asyncio.run(main())